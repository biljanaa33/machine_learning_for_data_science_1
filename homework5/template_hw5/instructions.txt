[PART 1]

Implement two regression methods, (1) kernelized ridge regression and (2) Support Vector Regression (SVR), and two kernels:

Polynomial kernel 
RBF kernel 
Implement SVR by solving the optimization problem in Eq. (10) from (Smola and Scholkopf, 2004) with cvxopt.solvers.qp. Inputs to qp should be represented so that the solution x contains 
 and 
 in the following order: 
. Set 
 as 
. To obtain 
, use the output y from cvxopt.solvers.qp. 
 could, in theory, also be obtained from Eq. (16), but it is very sensitive to inaccuracies (therefore, do not use it; the equation also contains an error).

Apply both regression methods and both kernels to the 1-dimensional sine data set. For each method/kernel find kernel and regularization parameters that fit well. For SVR, also take care to produce a sparse solution. This part aims to showcase what kernels can do and introduce the meaning of parameters. No need to do any formal parameter selection (such as with cross-validation) here. Plot the input data, the fit, and mark support vectors on the plot.

[PART 2, grades 7-8]

Sensibly apply both regression methods and both kernels to the housing2r data set. For each method/kernel, measure predictive performance with MSE and plot it versus a kernel parameter value (for polynomial kernel, M 
 
, for RBF choose interesting values of 
 yourself). Take care to set 
 properly. Plot two curves for each kernel/method, one with regularization parameter 
, and the other with 
 set with internal cross validation (for each kernel parameter value separately). For SVR, also display the number of support vectors for each score and try to keep it to a minimum while still getting a good fit.

Compare results between kernelized ridge regression and SVR and comment on the differences and similarities. Which learning algorithm would you prefer and why?

[PART 3, grades 9-10]

Find (or create) a regression data set with structured data: data with no natural description in a single table, such as text, images, graphs, or sound. Implement a kernel for the chosen data type and use it with either the kernelized ridge regression or SVR. Compare the resulting kernel to the naive attribute representation of your data set.